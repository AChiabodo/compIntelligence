{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of a Policy search is a \"black box\" that contains a set of rules to follow to play the game.  \n",
    "The outcome is NOT deterministic, because it vary depending on the opponent. Example of policy search are the min-max algorithm and the LCS algorithm.  \n",
    "A downside of min-max is that the opponent is assumed to be optimal, and minimizing the worst possible outcome is not always the best strategy, for example in investing that should be to not invest at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Classifier Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Classifier Systems (LCS): A Machine Learning Paradigm**\n",
    "\n",
    "Learning Classifier Systems (LCS) are a sophisticated machine learning paradigm that integrate the principles of reinforcement learning and genetic algorithms. Introduced by **John Holland**, LCS are founded on the concept of trigger-action rules that enable an agent to learn and adapt through interaction with its environment.\n",
    "\n",
    "**Core Components of LCS:**\n",
    "- **Input Interface**: Transforms real-world observations into a structured format that the system can process.\n",
    "- **Message List**: A comprehensive database of \"facts\" or observations derived from the input interface.\n",
    "- **Classifier List**: A dynamic set of condition-action rules that guide decision-making. Each rule follows the format \"IF condition THEN action,\" where conditions are derived from the message list, and actions correspond to possible responses.\n",
    "- **Output Interface**: Executes actions in the real world based on the facts present in the message list.\n",
    "\n",
    "**Example Rule**: \"IF temperature > 100 THEN there_is_fire\"\n",
    "\n",
    "**Evolutionary Mechanism**:\n",
    "- LCS utilize Genetic Algorithms (GA) to evolve the set of rules over time. This process involves reproduction and recombination of rules to adapt the agent's behavior based on feedback, typically in the form of numerical rewards.\n",
    "\n",
    "**Challenges in LCS**:\n",
    "- **Rule Creation**: Determining the initial set of rules that accurately represent the problem space.\n",
    "- **Rule Updating**: Evolving the rules to improve performance, guided by a well-defined fitness function that evaluates the utility of each rule.\n",
    "\n",
    "**Applications of LCS**:\n",
    "- LCS have been applied to a wide range of domains, including behavior modeling, classification, data mining, regression, function approximation, and game strategy, demonstrating their versatility in tackling complex problems¹.\n",
    "\n",
    "**Advancements in LCS**:\n",
    "- Modern LCS implementations, such as the XCS algorithm, have expanded the capabilities of LCS by incorporating accuracy-based fitness measures and complete action mapping, allowing for more precise and comprehensive learning¹."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is a prominent topic in the field of machine learning today. In RL, an agent performs actions that alter the environment and receives a reward for each action.  \n",
    "The primary objective is to maximize the total reward. With each action, we obtain a new state of the environment and a corresponding reward. \n",
    "\n",
    "The fundamental concept behind RL is to formalize sequential decision-making, with the aim of maximizing the **sum of rewards**.\n",
    "\n",
    "At each time step $t$, the agent:\n",
    "- Receives an observation $O_t$\n",
    "- Receives a reward $R_t$\n",
    "- Chooses an action $A_t$\n",
    "\n",
    "Simultaneously, at each time step $t$, the environment:\n",
    "- Receives an action $A_t$\n",
    "- Emits an observation $O_{t+1}$\n",
    "- Emits a reward $R_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of the environment, denoted as $ S_t^E $, is a function of the environment's history. However, this \"real\" environment state might not be fully observable by the agent.\n",
    "\n",
    "The agent's state, $ S_t^A $, is a function of the agent's history.\n",
    "\n",
    "An environment is considered **fully observable** if $ O_t = S_t^E = S_t^A $. In other words, the agent can directly observe the entire state of the environment. On the other hand, an environment is **partially observable** if the agent indirectly receives the state of the environment. An example of this is in games like poker, where not all cards are visible to all players.\n",
    "\n",
    "The history of the system, denoted as $ H_n $, represents the sequence of states, actions, and rewards over time. It can be expressed as follows: $$ H_n = \\{S_0 , A_0\\} , \\{S_1 , A_1 , R_1\\} , ... $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Expected Return* is the sum of all the rewards that an agent anticipates receiving from the current time step $ t $ to the \"final step\". This expectation is based on the agent's knowledge of the environment and its current history, and can be expressed as: $$ G _t = \\sum_{u=t}^{T} R_{u+1} $$  \n",
    "\n",
    "The agent's objective can be framed as **maximizing the expected return**. \n",
    "\n",
    "In some scenarios, there is a **scarcity of rewards**: the agent does not receive a reward at every step, but perhaps after a certain period or number of steps. \n",
    "\n",
    "There are two main types of tasks in reinforcement learning: \n",
    "\n",
    "- **Continuous tasks**: In these tasks, there is no \"final step\" and the expected return is infinite. Therefore, we introduce a \"discount rate\" $ \\gamma $, which defines how much we value future rewards. For instance, $ \\gamma = 0.9 $ implies that we value future rewards almost as much as the current reward, whereas $ \\gamma = 0.1 $ indicates that we place more value on the current reward. The problem can now be reframed as **maximizing the discounted rewards**, expressed as: $$ G _t = \\sum_{k=1}^{\\infty} \\gamma^k R_{k+1} $$\n",
    "\n",
    "- **Episodic tasks**: In these tasks, the agent receives a reward at the end of each episode. The same concept of discounted return can be applied, but now we have a \"final step\" and the expected return is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Rewards Hypothesis* posits that all goals can be framed as the maximization of the expected cumulative reward. This perspective implies that each action may have long-term consequences. \n",
    "\n",
    "In scenarios where rewards are delayed, it might be preferable to sacrifice short-term rewards to secure a larger reward in the future. For instance, an airplane might be loaded with extra fuel to prevent a crash if the journey takes longer than expected.\n",
    "\n",
    "There are two primary types of rewards:\n",
    "\n",
    "- **Dense Rewards**: The agent receives a reward at each step. For example, in chess, we could assign a reward for each move that does not result in a loss.\n",
    "- **Sparse Rewards**: The agent receives a reward only at the end of the episode. For instance, in chess, we could assign a reward only if the agent wins the game.\n",
    "\n",
    "*Time-Delayed Labeling*, also known as semi-supervised learning, is a scenario where the agent receives labels only after a certain amount of time, necessitating learning from unlabeled data. \n",
    "\n",
    "This is also referred to as the \"credit assignment problem\" (Minsky, 1961): how to correctly assign credit to each action that led to the reward when we only know the final outcome of a sequence of actions. It may not be clear what the \"real\" sequence of actions leading to the reward was.\n",
    "\n",
    "Examples of rewards include:\n",
    "\n",
    "- Managing a Portfolio: $ \\pm r $ for each € gained or lost while managing the portfolio.\n",
    "- Controlling a Power Station: $ +r $ for power produced while remaining within the safe zone, $ -r $ if the station exceeds the safe zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Agent's Policy** defines an agent's behavior. It is a mapping from the agent's state to the agent's action. It can be defined as:\n",
    "\n",
    "- **Deterministic Policy**: $ \\pi (s) = a $, where $ \\pi $ is the policy, $ a $ is the action, and $ s $ is the state.\n",
    "- **Stochastic Policy**: $ \\pi (a|s) = P[A_t = a | S_t = s] $, where $ \\pi $ is the policy, $ a $ is the action, and $ s $ is the state.\n",
    "\n",
    "It's important to note that the state should also contain some information about the agent's history.\n",
    "\n",
    "The **Markov Hypothesis** posits that, given sufficient information, the future can be predicted without knowing the entire past. In other words, the future is independent of the past given the present. This means that if we have complete knowledge of the current state, then the future can be predicted independently of the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample efficiency** : how many samples are needed to learn a good policy. Very important in reinforcement learning. \n",
    "\n",
    "There are main different approaches to optimize a policy :  \n",
    "- Differential Programming\n",
    "- Monte Carlo Methods : not very sample-efficient method\n",
    "- Temporal Difference Learning\n",
    "- Bellman Equations \n",
    "- Exploration vs Exploitation\n",
    "- Gradient Descent : EA & co. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperfect Information Games\n",
    "\n",
    "Imperfect Information Games are a specific category of games where each player does not have complete knowledge of the game's full state.  \n",
    "To address this challenge, one approach is to treat each unknown piece of information as a random variable and compute the probability of each possible state. \n",
    "\n",
    "Another strategy is to assume the worst possible hidden information, placing us in the most unfavorable scenario. This is linked to the minmax algorithm.  \n",
    "In such cases, we can employ the \"Expected Minimax\" algorithm, which calculates the expected value of the minmax algorithm.\n",
    "\n",
    "If the simulation becomes too intricate, we can halt at a certain depth and adopt a \"random\" policy. This results in a Monte Carlo Tree Search.  \n",
    "**Monte Carlo Tree Search (MCTS)**: In MCTS, we select a leaf node and expand it. We then simulate a random game from that node and backpropagate the result, updating all preceding nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Hypothesis\n",
    "\n",
    "In scenarios where rewards are sparse and/or delayed, we may encounter difficulties in learning an effective policy. This is often referred to as the credit-assignment problem.\n",
    "\n",
    "The **Agent's Policy** defines an agent's behavior. It is a **mapping from the agent's state to the agent's action** and can be either a deterministic function or a stochastic policy.\n",
    "\n",
    "The value function $ v_\\pi(s)$ under policy $\\pi$ is the expected return when starting from state $s$ and following policy $\\pi$. It is defined as: $$ v_\\pi(s) = E_\\pi[G_t | S_t = s] = E_\\pi[ \\sum_{k=0}^{T-t+1} \\gamma^k R_{t+k+1} | S_t = s]$$  \n",
    "\n",
    "The State-value Function, $ v_*(s)$, represents the best possible value obtainable when starting from state $s$ and following policy $\\pi$: $$ v_*(s) = max_\\pi v_\\pi(s) $$  \n",
    "\n",
    "The state-value function can be decomposed using the Bellman Equation.\n",
    "\n",
    "$q_\\pi$ denotes the action-value function under policy $\\pi$: $$ q_\\pi(s,a) = E_\\pi[G_t | S_t = s, A_t = a] = E_\\pi[ \\sum_{k=0}^{T-t+1} \\gamma^k R_{t+k+1} | S_t = s, A_t = a]$$  \n",
    "\n",
    "The optimal action is the one that yields the highest value: $$ q_*(s,a) = max_\\pi q_\\pi(s,a) $$  \n",
    "\n",
    "Knowing the optimal action-value function allows us to determine the optimal policy: $$ \\pi_*(s) = argmax_a q_*(s,a) $$  \n",
    "\n",
    "There are two main types of reinforcement learning:\n",
    "\n",
    "- **Model-based**: Here, we have a **model of the environment** which we can use to predict the next state and the reward. We can use the model to plan the next action. These methods follow the **Markov Decision Process**. The model can be used to learn the policy. An example of this is model-based Q-learning.\n",
    "- **Model-free**: In this case, we don't have a model of the environment and we need to learn the policy directly from experience. We can use the experience to learn the policy. An example of this is the gradient-free Monte Carlo approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO Rewrite this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Reinforcement Learning\n",
    "\n",
    "Model-based Reinforcement Learning can be exemplified by training an agent to solve a maze, where a reward of -1 is given at each step taken by the algorithm.\n",
    "\n",
    "A **Markov State** is a state where the future is independent of the past, given the present. In other words, the current state is a sufficient statistic of the future. This can be expressed as: $$ P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] $$  \n",
    "\n",
    "A **Markov Process** is a tuple $<S,P>$, where $S$ is a set of states and $P$ is a state transition probability matrix.\n",
    "\n",
    "A **Markov Reward Process** is a tuple $<S,P,R,\\gamma>$, where $S$ is a set of states, $P$ is a state transition probability matrix, $R$ is a reward function defined as $R(s) = E[R_{t+1}|S_t = s]$, and $\\gamma$ is a discount factor.\n",
    "\n",
    "**Markov Decision Processes (MDPs)** provide an environment for reinforcement learning. They are fully observable, and the current state completely characterizes the process. It's worth noting that most reinforcement learning problems can be formalized as MDPs.\n",
    "\n",
    "An **MDP** is a tuple $<S,A,P,R,\\gamma>$, where:\n",
    "- $S$ is a set of states\n",
    "- $A$ is a set of actions\n",
    "- $P$ is a state transition probability matrix $p(s'|s,a)$, which represents the state space probability distribution given that action $a$ is taken from state $s$.\n",
    "- $R$ is a reward function defined as $R(s) = E[R_{t+1}|S_t = s]$ = S x A\n",
    "- $\\gamma$ is a discount factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-free Reinforcement Learning\n",
    "Monte Carlo methods : we don't have a model of the environment, but we can learn the policy directly from the experience just by doing a random sequence of moves. In this way we may end up learning both the environment and ther value function.  \n",
    "The Montecarlo method is an episodic learning algorithm where the total reward after each episode is used to update the value function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
