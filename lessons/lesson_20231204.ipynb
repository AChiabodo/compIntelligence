{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of a Policy search is a \"black box\" that contains a set of rules to follow to play the game.  \n",
    "The outcome is NOT deterministic, because it vary depending on the opponent. Example of policy search are the min-max algorithm and the LCS algorithm.  \n",
    "A downside of min-max is that the opponent is assumed to be optimal, and minimizing the worst possible outcome is not always the best strategy, for example in investing that should be to not invest at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Classifier Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not related with today classifiers. They're based on trigger-action rules. They're a machine learning system with close links to reinforced learning and *genetic algorithm*.\n",
    "We can see LCS as a framework that uses genetic algorithms to study learning conditions in rule-based systems. As of today they're completely outdated and useless. \n",
    "Create a certain number of rules where we have a tuple (condition,action) and we can \"learn\" those.\n",
    "Costituited of:\n",
    "- Input Interface : creating some kind of input starting from the real world, and providing a \"valid\" description about that\n",
    "- message list : big databases of \"facts\"\n",
    "- Classifier List : set of **rules** that are used to take decisions, composed of IF condition AND condition (...) THEN action. The conditions are usually a subset of the message list. The action is usually a subset of the possible actions. For example \"IF squillero_talking AND is_thursday THEN squillero_is_teaching\". That's not about probabilities, but about facts and rules.\n",
    "- Output Interaface : If there is some facts in the message queues, then do something in the real world.  \n",
    "For example \"IF there_is_fire THEN call_firefighters\".In this case we can have an input interface that read temperature, a Classifier List that contains \"IF temperature > 100 THEN there_is_fire\" and an output interface that calls firefighters.  \n",
    "\n",
    "The main problems in LCS are about how to create the rules and how to update them. Holland propose to have a GA that can update the existing rules. Now the problem is to define a **fitness** function to evaluate a rule. Note that the rule-set represent the current knowledge of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the \"big topics\" in today machine learning, where an Agent is performing actions that **modify the environment** and receive a **reward** for each action. The goal is to maximize the reward. At each step we obtain a \"new\" state of the environment and a reward.  \n",
    "Considered a subfield of machine learning and AI.  \n",
    "The main idea behind RL is to formalize a sequential decision making where the goal is to maximize the **sum of rewards**.\n",
    "At each step $ t $ the agent :\n",
    "- receives an observation $ O_t $\n",
    "- receives a reward $ R_t $\n",
    "- chooses an action $ A_t $\n",
    "At each step $ t $ the environment:\n",
    "- receives an action $ A_t $\n",
    "- emits an observation $ O_{t+1} $\n",
    "- emits a reward $ R_{t+1} $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment state is a function of the history of the environment. But the \"real\" Environment State $ S_t^E $ might not be fully visible to the agent.\n",
    "The agent state $ S_t^A $ is a function of the history of the agent.  \n",
    "An environment is **fully observable** if $ O_t = S_t^E = S_t^A $, otherwise it's **partially observable** where the agent receives \"indirectly the environment, like in pokers.  \n",
    "The history of the system represents the sequence of observations, actions and rewards that happened over time : $$ H_n = {S_0 , A_0} , {S_1 , A_1 , R_1} , ... $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Expected Return* is the sum of all the rewards that we expect to receive from the current time $ t $ to the \"final step\" based on our knowledge of the environment and the current history: $$ G _t = \\sum_{u=t}^{T} R_{u+1} $$  \n",
    "The goal of the agent can be casted to **maximize the expected return**.\n",
    "**Scarcity of rewards** : the agent does not receive a reward at each step, but maybe after some time or after a certain number of steps.  \n",
    "There are two main types of tasks:  \n",
    "- **Contuinuous tasks** : there is no \"final step\" and the expected return is infinite, so we need to introduce a \"discount rate\" $ \\gamma $ that defines how much we value future rewards. For example $ \\gamma = 0.9 $ means that we value future rewards as much as the current reward , on the other hand $ \\gamma = 0.1 $ means that we value more the current reward. The problem can now be casted as **maximize the discounted rewards**\n",
    "$$ G _t = \\sum_{k=1}^{\\infty} \\gamma^k R_{k+1} $$\n",
    "- **Episodic tasks** : the agent receives a reward at the end of each episode. The same discounted return can be used, but now we have a \"final step\" and the expected return is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards hypothesis : all the goals can be described by the maximization of the expected cumulative reward. In this way each action may have long term consequences.  \n",
    "In the case that rewards are delayed, we can prefer to sacrifice the short term reward to obtain a bigger reward in the future, for example putting more fuel in an airplane to prevent it crashing if it needs more time to arrive at destination.  \n",
    "\n",
    "There are two main types of rewards:\n",
    "- **Dense rewards** : the agent receives a reward at each step. For example in chess we can give a reward for each move that is not a losing move.\n",
    "- **Sparse rewards** : the agent receives a reward only at the end of the episode. For example in chess we can give a reward only if the agent wins the game.\n",
    "\n",
    "Time-delayed labeling (aka semi-supervised learning) : the agent receives the labels only after a certain amount of time, so sometimes needs to learn from unlabeled data.  \n",
    "Also called as the \"credit assignment problem\" (Minsky, 1963) : how to assign the correct credit to each action that led to the reward if we only know that at the end of a certain amount of actions involved. We may not know what was the \"real\" sequence of actions that led to the reward.\n",
    "\n",
    "Examples of rewards:\n",
    "- Managing a portfolio : $ \\pm r $ for each â‚¬ gained or lost while managing the portfolio\n",
    "- Controlling a power station : $ +r $ for power produced while remaining in the safe zone, $ -r $ if we go out of the safe zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent's Policy** : The definition of an Agent's behavior. It is a mapping from the agent's state to the agent's action.   \n",
    "Defined as :  \n",
    "- Deterministic Policy : $ \\pi (s) = a $\n",
    "- Stochastic Policy : $ \\pi (a|s) = P[A_t = a | S_t = s] $  \n",
    "where $ \\pi $ is the policy, $ a $ is the action and $ s $ is the state.\n",
    "  \n",
    "Note that the state should contain also some information about the history of the agent.  \n",
    "\n",
    "**Markov Hypothesis** : given enough informations, I can predict the future without knowing all the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample efficiency** : how many samples are needed to learn a good policy. Very important in reinforcement learning. \n",
    "\n",
    "There are main different approaches to optimize a policy :  \n",
    "- Differential Programming\n",
    "- Monte Carlo Methods : not very sample-efficient method\n",
    "- Temporal Difference Learning\n",
    "- Bellman Equations \n",
    "- Exploration vs Exploitation\n",
    "- Gradient Descent : EA & co. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperfect Information Games\n",
    "Particular case of games where each player does not know the full state of the game.  \n",
    "One way to solve this problem is to suppose that each \"not known\" information is a random variable and we can compute the probability of each possible state.  \n",
    "Another way is to suppose the worst possible hidden information and then we're in the worst possible scenario, linked to minmax algorithm.  \n",
    "In this case we can use th \"Expected Minimax\" algorithm, where we compute the expected value of the minmax algorithm.  \n",
    "If the simulation is too complex, we can stop at a certain depth and use a \"random\" policy, we've obtained a Monte Carlo Tree Search.\n",
    "**Monte Carlo Tree Search** : We select a leaf node and we expand it, then we simulate a random game from that node and we backpropagate the result updating all the previous nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment : find an (easy) example where reinforcement learning does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Hypothesis\n",
    "In case of sparse rewards and/or delayed rewards we can have problems in learning a good policy, also called the credit-assignment problem.\n",
    "**Agent's policy** : the definition of an agent's behavior. It is a **mapping from the agent's state to the agent's action**. Can be a deterministic function or a stochastic policy. \n",
    "The value function $ v_\\pi(s)$ under the policy $\\pi$ is defined as the expected return starting from state $s$ and following policy $\\pi$ : $$ v_\\pi(s) = E_\\pi[G_t | S_t = s] = E_\\pi[ \\sum_{k=0}^{T-t+1} \\gamma^k R_{t+k+1} | S_t = s]$$  \n",
    "The State-value Function is defined as the best possible value that we can obtain starting from state $s$ and following policy $\\pi$ : $$ v_*(s) = max_\\pi v_\\pi(s) $$  \n",
    "The state-value function can be decomposed as a Bellman Equation ...  \n",
    "$q_\\pi$ is the action-value function under policy $\\pi$ : $$ q_\\pi(s,a) = E_\\pi[G_t | S_t = s, A_t = a] = E_\\pi[ \\sum_{k=0}^{T-t+1} \\gamma^k R_{t+k+1} | S_t = s, A_t = a]$$  \n",
    "The optimal action is the action that gives me the higher value : $$ q_*(s,a) = max_\\pi q_\\pi(s,a) $$  \n",
    "So knowing the optimal action-value function we can find the optimal policy : $$ \\pi_*(s) = argmax_a q_*(s,a) $$  \n",
    "\n",
    "We can recognize two different types of reinforcement learning :\n",
    "- **Model-based** : we have a **model of the environment** and we can use it to predict the next state and the reward. We can use the model to plan the next action. These methods follows the **Markov Decision Process**. We can use the model to learn the policy. An example is the model-based Q-learning.\n",
    "- **Model-free** : we don't have a model of the environment and we need to learn the policy directly from the experience. We can use the experience to learn the policy. An example is the gradient free Montecarlo approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Reinforcement Learning\n",
    "An example could be training an agent to solve a maze. We're giving -1 at each step taken by the algorithm.  \n",
    "Markov State : the future is unknown, but independent of the past given the present. In other words the current state is a sufficient statistic of the future.  \n",
    "$$ P[S_{t+1} | S_t] = P[S_{t+1} | S_1, ..., S_t] $$  \n",
    "A Markov Process is a tuple $<S,P>$ where $S$ is a set of states and $P$ is a state transition probability matrix.  \n",
    "A Markov Reward Process is a tuple $<S,P,R,\\gamma>$ where $S$ is a set of states, $P$ is a state transition probability matrix, $R$ is a reward function $R(s) = E[R_{t+1}|S_t = s]$ and $\\gamma$ is a discount factor.  \n",
    "Markov Decision Process are an environment for reinforced learning. Which is fully observable and the current state completely characterizes the process. Note that most of the RL problems can be formalized as MDPs.  \n",
    "A Markov Decision Process is a tuple $<S,A,P,R,\\gamma>$ where:\n",
    "- $S$ is a set of states\n",
    "- $A$ is a set of actions\n",
    "- $P$ is a state transition probability matrix $p(s'|s,a)$ which is the state space probability distribution given that from state $s$ we take action $a$.\n",
    "- $R$ is a reward function $R(s) = E[R_{t+1}|S_t = s]$ = S x A\n",
    "- $\\gamma$ is a discount factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-free Reinforcement Learning\n",
    "Monte Carlo methods : we don't have a model of the environment, but we can learn the policy directly from the experience just by doing a random sequence of moves. In this way we may end up learning both the environment and ther value function.  \n",
    "The Montecarlo method is an episodic learning algorithm where the total reward after each episode is used to update the value function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
