{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Search and Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The study of games is essential because they are often **simple to formalize** and serve as **effective models** for real-world scenarios involving competitive or cooperative dynamics. Additionally, games provide an excellent platform for testing artificial intelligence (AI) methodologies.\n",
    "\n",
    "The primary objective in adversarial search is to develop an **optimal policy** for an agent. This policy can be visualized as a \"black box\" that receives the current state as input and outputs the subsequent action to execute. Another aim is to determine the **optimal ply**—the best move in response to the opponent's action—for each state.\n",
    "\n",
    "#### Why Study Games in AI?**\n",
    "\n",
    "Games are **intuitive models** for real-world problems featuring competitive or cooperative elements and serve as a robust testing ground for AI techniques.\n",
    "\n",
    "**Key Objectives:**\n",
    "- **Optimal Policy**: A strategy that dictates the best action for an agent based on the current state.\n",
    "- **Optimal Ply**: The ideal countermove in reaction to an opponent's move.\n",
    "\n",
    "**Game Types:**\n",
    "- **Deterministic**: The next state is fully determined by the current state and the agent's action. Example: Chess.\n",
    "- **Probabilistic**: The next state depends on the current state, the agent's action, and a random event. Example: Backgammon, Poker.\n",
    "- **Real-Time**: The agent must decide actions within a limited timeframe. Example: First-Person Shooter (FPS) games.\n",
    "- **Turn-Based**: The agent has no time constraints to decide actions. Example: Chess.\n",
    "- **Perfect Information**: The agent has full knowledge of the game's state. Example: Chess.\n",
    "- **Imperfect Information**: The agent lacks complete knowledge of the game's state. Example: Poker.\n",
    "- **Zero-Sum**: One player's gain is another player's loss. Example: Chess.\n",
    "- **Non-Zero-Sum**: A player's gain does not necessarily result in another player's loss. Example: Soccer.\n",
    "\n",
    "**Examples:**\n",
    "- Chess is a deterministic, turn-based, perfect information, zero-sum game.\n",
    "- Poker is a probabilistic, turn-based, imperfect information, zero-sum game.\n",
    "- Nuclear war is a probabilistic, real-time, imperfect information, non-zero-sum game. The same can be humorously said for marriage.\n",
    "\n",
    "**Differences Between Games and Search Problems:**\n",
    "\n",
    "Games and search problems, while similar in their use of algorithms and strategies, exhibit key differences:\n",
    "\n",
    "1. **Predictability of Opponent's Play**: In games, the opponent's strategy is unknown, making the goal to devise a perfect strategy (policy) to win. In contrast, search problems have a defined goal, and the task is to find the optimal path to that goal.\n",
    "\n",
    "2. **Importance of Efficiency**: Time is a critical factor in games; finding the best move quickly is essential. Moreover, games typically have a higher branching factor than search problems, making pruning strategies crucial to eliminate futile moves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deterministic Games\n",
    "\n",
    "Deterministic games involve a defined number of players, a set of states representing various situations, a collection of possible actions, a transition model, a terminal test, and a utility function. The terminal test or function is used for a static evaluation of terminal states to determine a win or loss.\n",
    "\n",
    "A **game tree** is a theoretical model used in game theory and artificial intelligence to represent possible moves in a game. Each node symbolizes a unique state of the game, while each edge denotes a potential action that transitions from one state to another. The root node corresponds to the initial state of the game, often right after a move has been made, and the leaf nodes represent the terminal states where the game concludes.\n",
    "\n",
    "**Key Properties of a Game Tree:**\n",
    "- **Branching Factor**: This refers to the average number of child nodes for each node in the tree, indicating the average number of possible actions from any given state.\n",
    "- **Depth**: The depth of the tree reflects the number of layers, which correlates to the number of moves that have been played in the game.\n",
    "- **Symmetries**: Symmetries in a game tree reveal the presence of identical nodes, which can occur due to the repetitive nature of certain games.\n",
    "\n",
    "Consider the game of tic-tac-toe:\n",
    "- The root node is the empty board.\n",
    "- The first layer consists of all possible moves by the first player.\n",
    "- The second layer contains all possible responses by the second player, and so on.\n",
    "\n",
    "In tic-tac-toe, a strategy aimed at maximizing the probability of winning might involve starting with one of the corner squares. If we play second against a \"perfect\" strategy, our best outcome is a draw by countering every move of the opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Minimax Algorithm\n",
    "\n",
    "The **minimax algorithm** is a well-known strategy for turn-based games where two players, often referred to as **Max** and **Min**, compete against each other.  \n",
    "Max aims to maximize the utility function, while Min seeks to minimize it.  \n",
    "The algorithm's purpose is to **minimize the maximum possible loss** in scenarios where losing is inevitable.\n",
    "\n",
    "It's crucial to understand that maximizing the chances of winning is not equivalent to minimizing potential losses.\n",
    "\n",
    "**Key Aspects:**\n",
    "- The algorithm typically employs a recursive approach with alternating signs at each level, effectively maximizing the utility function from the perspective of the current player.\n",
    "- It is a complete and optimal strategy only if the game tree is finite and both players are playing optimally with the intent to win.\n",
    "- The computational complexity is \\( O(b^m) \\) in time and \\( O(b \\cdot m) \\) in space, where \\( b \\) is the branching factor and \\( m \\) is the maximum depth of the tree.\n",
    "\n",
    "**Adapting to Different Scenarios:**\n",
    "- The minimax algorithm can be adapted for games with more than two players by adding additional layers and modifying the sign-flipping mechanism.\n",
    "- For games with vast state spaces, such as chess with approximately \\( 10^{120} \\) possible states, the minimax algorithm becomes impractical due to its complexity.\n",
    "\n",
    "**Transforming Problems:**\n",
    "- Games like tic-tac-toe can be reformulated into different representations, such as the \"Pick 3 numbers whose sum is 12\" game, to simplify the visualization of the game tree and determine winning states.\n",
    "\n",
    "```python\n",
    "def won(cells):\n",
    "    return any(sum(cells[i] for i in combo) == 12 for combo in combos)\n",
    "\n",
    "def minimax(board):\n",
    "    val = eval_terminal(*board) # 0 if not terminal, 1 if won, -1 if lost\n",
    "    possible = list(set(range(9)) - board[0] - board[1])\n",
    "    if val != 0 or not possible:\n",
    "        return None, val\n",
    "    evaluatiations = []\n",
    "    for ply in possible:\n",
    "        new_board = (board[1], board[0] | {move})\n",
    "        _, val = minimax(new_board)\n",
    "        evaluations.append((move, val))\n",
    "    return max(evaluations, key=lambda x: x[1])\n",
    "```\n",
    "\n",
    "The major limitation of the minimax algorithm is that it is **computationally infeasible** for games with large state spaces. For example, the game of chess has a branching factor of approximately 31 and a maximum depth of 100, resulting in a state space of \\( 31^{100} \\). This is far too large to compute in a reasonable amount of time.\n",
    "\n",
    "**Limitations of the Minimax Algorithm**\n",
    "\n",
    "The **minimax algorithm** faces significant challenges when applied to games with extensive state spaces due to its computational demands. For instance, the game of chess, which is often used to benchmark AI algorithms, has an estimated state-space complexity of \\( 10^{46} \\) and a game tree complexity of \\( 10^{123} \\), based on an average branching factor of 35 and an average game length of 80 ply⁶. These figures highlight the impracticality of using the minimax algorithm for exhaustive search in such complex games.\n",
    "\n",
    "To address these limitations, various enhancements and alternatives to the minimax algorithm have been developed, such as **alpha-beta pruning**, which reduces the number of nodes evaluated by the algorithm, and **iterative deepening**, which allows the algorithm to use a more manageable depth of search¹."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha-Beta pruning\n",
    "To avoid to explore the entire tree we can use a technique called **Alpha-Beta pruning**. \"If you have an idea that is surely bad, don't waste time to see how bad it is\". In this way we can avoid to explore some branches of the tree that cointain only \"bad\" (or useless) states.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chess and AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First automaton built in 1912 by Leonardo Torres y Quevedo. Only able to win with a king and a rook vs a King. Not optimal nor complete.\n",
    "Alan Turing then develope the Turochamp in 1948. Based on very basic rules and a lot of pruning. It was able to solve mate in 2 problems.\n",
    "In 1950 Claude Shannon wrote a paper about how to program a computer to play chess. Given the size of all the possible states of the game ( about $ 10^120 $) , he suggested to use static evaluation of the current state of the game.\n",
    "\n",
    "deep blue vs kasparov : 1997, first time that a computer won against a world champion. Based on the work of Shannon, pruning, and a bit of machine learning (other than a LOT of hardware).  \n",
    "\n",
    "A main characteristic of chess engine is the **hard cut off**. It's a limit on the depth of the tree that the engine can explore. It's used to avoid to spend too much time on a single move and making the problem feasible in a certain amount of time.\n",
    "*Horizon Effect*: when the engine incorrectly estimates the value of a move beacause it's just beyond it's hard cut off.  \n",
    "*Quiescence search* : increase dept of the search when a move is *volatile* (when its value changes a lot from level to level).We need to be able to discriminate between \"quiet\" and \"queiscent\" positions.  \n",
    "Possible techniques to improve the performance of a chess engine:\n",
    "-  **Hash Table** : store the value of a static position that has been already evaluated. In this way we can avoid to re-evaluate the same position multiple times.\n",
    "-  **Lookup tables** : Store the value of all known opening moves and endgames. In this way we can avoid to evaluate them and already know what to use. For endgame we'll nee to find a canonical representation of the board to store them.  \n",
    "\n",
    "Machine Learning can be used to \"learn\" the value of certain position to be able to evaluate them without the need of a static evaluation function.\n",
    "\n",
    "#### Stochastic games\n",
    "How can we incorporate randomness in our game tree? We can modify the minmax algorithm to take into account the probability of each action and then try to maximize the *expected* reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of a Policy search is a \"black box\" that contains a set of rules to follow to play the game.  \n",
    "The outcome is NOT deterministic, because it vary depending on the opponent. Example of policy search are the min-max algorithm and the LCS algorithm.  \n",
    "A downside of min-max is that the opponent is assumed to be optimal, and minimizing the worst possible outcome is not always the best strategy, for example in investing that should be to not invest at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Classifier Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not related with today classifiers. They're based on trigger-action rules. They're a machine learning system with close links to reinforced learning and *genetic algorithm*.\n",
    "We can see LCS as a framework that uses genetic algorithms to study learning conditions in rule-based systems. As of today they're completely outdated and useless. \n",
    "Create a certain number of rules where we have a tuple (condition,action) and we can \"learn\" those.\n",
    "Costituited of:\n",
    "- Input Interface : creating some kind of input starting from the real world, and providing a \"valid\" description about that\n",
    "- message list : big databases of \"facts\"\n",
    "- Classifier List : set of **rules** that are used to take decisions, composed of IF condition AND condition (...) THEN action. The conditions are usually a subset of the message list. The action is usually a subset of the possible actions. For example \"IF squillero_talking AND is_thursday THEN squillero_is_teaching\". That's not about probabilities, but about facts and rules.\n",
    "- Output Interaface : If there is some facts in the message queues, then do something in the real world.  \n",
    "For example \"IF there_is_fire THEN call_firefighters\".In this case we can have an input interface that read temperature, a Classifier List that contains \"IF temperature > 100 THEN there_is_fire\" and an output interface that calls firefighters.  \n",
    "\n",
    "The main problems in LCS are about how to create the rules and how to update them. Holland propose to have a GA that can update the existing rules. Now the problem is to define a **fitness** function to evaluate a rule. Note that the rule-set represent the current knowledge of the system.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
